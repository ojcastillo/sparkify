{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Notebook\n",
    "\n",
    "> This notebook was developed as part of Udacity's Sparkify Capstone Project, which was one of the possible final projects to tackle in the Data Scientist Nanodegree Program (cohort of Aug-Dec 2019).\n",
    "\n",
    "Sparkify is a fictional service similar to [Spotify](https://en.wikipedia.org/wiki/Spotify). At a high level, users can come and play songs from a large pool of artists either as a guest or as a logged-in user. They can also decide to pay for the service for more benefits. They're also free to unsubscribe from the service at any time. \n",
    "\n",
    "Udacity's is graciously providing both a medium (128MB) and large (12GB) dataset with user activity events to play with. For example, a row might represent the action of a particular user playing a song from the artist \"Metallica\". We want to use this dataset to try to predict if an user will churn in the near future (in this case, unsubscribe from the service).\n",
    "\n",
    "Predicting churn is a challenging and common problem that data scientists and analysts regularly encounter in any customer-facing business. Additionally, the ability to efficiently manipulate large datasets with Spark is one of the highest-demand skills in the field of data.\n",
    "\n",
    "This work shows how [pyspark](https://pypi.org/project/pyspark/) can be used to craft a machine learning model for predicting user churn. For most of the notebook, the medium sized dataset is used just so we can craft the features needed and to gain confidence in a supervised learning model in a local machine, but results from running the model training and evaluation on the large dataset using [AWS EMR](https://aws.amazon.com/emr/) are shared towards the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Let's start by importing all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as sqlF\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's create the SparkSession we'll be using from this point onward and load the medium dataset for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session that will be used for the whole notebook\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read smaller sparkify dataset for the initial exploration phase\n",
    "\n",
    "# READ ME!!!\n",
    "# Use the following path instead if you're running from AWS\n",
    "# file_path = 's3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json'\n",
    "\n",
    "# Use the following path only when running locally and after downloading datasets\n",
    "file_path = './datasets/mini_sparkify_event_data.json'\n",
    "\n",
    "df = spark.read.json(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Dataset\n",
    "\n",
    "Let's start by gathering some high level fast about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema per entry in the json\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286500, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print how many rows and columns are in the dataset\n",
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_describe(df, jump_size=5):\n",
    "    \"\"\"A wrapper around describe that prints `jump_size` columns at a time\"\"\"\n",
    "    ncols = len(df.columns)\n",
    "    for idx in range(0, ncols, jump_size):\n",
    "        col_list = df.columns[idx:idx+jump_size]\n",
    "        print(f'Summary statistics for {col_list}')\n",
    "        df.describe(col_list).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for ['artist', 'auth', 'firstName', 'gender', 'itemInSession']\n",
      "+-------+------------------+----------+---------+------+------------------+\n",
      "|summary|            artist|      auth|firstName|gender|     itemInSession|\n",
      "+-------+------------------+----------+---------+------+------------------+\n",
      "|  count|            228108|    286500|   278154|278154|            286500|\n",
      "|   mean| 551.0852017937219|      null|     null|  null|114.41421291448516|\n",
      "| stddev|1217.7693079161374|      null|     null|  null|129.76726201141085|\n",
      "|    min|               !!!| Cancelled| Adelaida|     F|                 0|\n",
      "|    max| ÃÂlafur Arnalds|Logged Out|   Zyonna|     M|              1321|\n",
      "+-------+------------------+----------+---------+------+------------------+\n",
      "\n",
      "Summary statistics for ['lastName', 'length', 'level', 'location', 'method']\n",
      "+-------+--------+------------------+------+-----------------+------+\n",
      "|summary|lastName|            length| level|         location|method|\n",
      "+-------+--------+------------------+------+-----------------+------+\n",
      "|  count|  278154|            228108|286500|           278154|286500|\n",
      "|   mean|    null|249.11718197783722|  null|             null|  null|\n",
      "| stddev|    null| 99.23517921058324|  null|             null|  null|\n",
      "|    min|   Adams|           0.78322|  free|       Albany, OR|   GET|\n",
      "|    max|  Wright|        3024.66567|  paid|Winston-Salem, NC|   PUT|\n",
      "+-------+--------+------------------+------+-----------------+------+\n",
      "\n",
      "Summary statistics for ['page', 'registration', 'sessionId', 'song', 'status']\n",
      "+-------+-------+--------------------+-----------------+--------------------+------------------+\n",
      "|summary|   page|        registration|        sessionId|                song|            status|\n",
      "+-------+-------+--------------------+-----------------+--------------------+------------------+\n",
      "|  count| 286500|              278154|           286500|              228108|            286500|\n",
      "|   mean|   null|1.535358834085557E12|1041.526554973822|            Infinity|210.05459685863875|\n",
      "| stddev|   null| 3.291321616328068E9|726.7762634630834|                 NaN| 31.50507848842202|\n",
      "|    min|  About|       1521380675000|                1|\u001c",
      "ÃÂg ÃÂtti Gr...|               200|\n",
      "|    max|Upgrade|       1543247354000|             2474|ÃÂau hafa slopp...|               404|\n",
      "+-------+-------+--------------------+-----------------+--------------------+------------------+\n",
      "\n",
      "Summary statistics for ['ts', 'userAgent', 'userId']\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|summary|                  ts|           userAgent|            userId|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|  count|              286500|              278154|            286500|\n",
      "|   mean|1.540956889810471...|                null| 59682.02278593872|\n",
      "| stddev|1.5075439608187113E9|                null|109091.94999910519|\n",
      "|    min|       1538352117000|\"Mozilla/5.0 (Mac...|                  |\n",
      "|    max|       1543799476000|Mozilla/5.0 (comp...|                99|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print descriptive statistics 5 columns at a time\n",
    "nice_describe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not missing any `userId`, but it looks like we have rows with empty values. Since we're interested in user churn, then ideally we want to be able to trace back each row to some user's action. Let's explore those rows and then make a decision about what do with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with empty userId: 8346\n",
      "Sample of rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(artist=None, auth='Logged Out', firstName=None, gender=None, itemInSession=100, lastName=None, length=None, level='free', location=None, method='GET', page='Home', registration=None, sessionId=8, song=None, status=200, ts=1538355745000, userAgent=None, userId=''),\n",
       " Row(artist=None, auth='Logged Out', firstName=None, gender=None, itemInSession=101, lastName=None, length=None, level='free', location=None, method='GET', page='Help', registration=None, sessionId=8, song=None, status=200, ts=1538355807000, userAgent=None, userId=''),\n",
       " Row(artist=None, auth='Logged Out', firstName=None, gender=None, itemInSession=102, lastName=None, length=None, level='free', location=None, method='GET', page='Home', registration=None, sessionId=8, song=None, status=200, ts=1538355841000, userAgent=None, userId='')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore rows with an empty user_id\n",
    "no_user_df = df.filter('userId == \"\"')\n",
    "print(f'Number of rows with empty userId: {no_user_df.count()}')\n",
    "print('Sample of rows:')\n",
    "no_user_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_stats(df, columns, sample_size=5):\n",
    "    \"\"\"Function to print unique value stats of specific columns\"\"\"\n",
    "    for col in columns:\n",
    "        print(f'\\nColumn \"{col}\":')\n",
    "        uniques = df.select(col).dropDuplicates()\n",
    "        nuniques = uniques.count()\n",
    "        print(f'\\tNumber of unique values: {nuniques}')\n",
    "        print(f'\\tSample: {uniques.head(sample_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column \"artist\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(artist=None)]\n",
      "\n",
      "Column \"auth\":\n",
      "\tNumber of unique values: 2\n",
      "\tSample: [Row(auth='Logged Out'), Row(auth='Guest')]\n",
      "\n",
      "Column \"firstName\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(firstName=None)]\n",
      "\n",
      "Column \"gender\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(gender=None)]\n",
      "\n",
      "Column \"lastName\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(lastName=None)]\n",
      "\n",
      "Column \"level\":\n",
      "\tNumber of unique values: 2\n",
      "\tSample: [Row(level='free'), Row(level='paid')]\n",
      "\n",
      "Column \"location\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(location=None)]\n",
      "\n",
      "Column \"method\":\n",
      "\tNumber of unique values: 2\n",
      "\tSample: [Row(method='PUT'), Row(method='GET')]\n",
      "\n",
      "Column \"page\":\n",
      "\tNumber of unique values: 7\n",
      "\tSample: [Row(page='Home'), Row(page='About'), Row(page='Submit Registration'), Row(page='Login'), Row(page='Register'), Row(page='Help'), Row(page='Error')]\n",
      "\n",
      "Column \"song\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(song=None)]\n",
      "\n",
      "Column \"userAgent\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(userAgent=None)]\n"
     ]
    }
   ],
   "source": [
    "# Print unique value statistics of all categorical columns for rows with no user_id defined\n",
    "categorical_cols = ['artist', 'auth', 'firstName', 'gender', 'lastName', 'level', \n",
    "                    'location', 'method', 'page', 'song', 'userAgent']\n",
    "show_unique_stats(no_user_df, categorical_cols, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no userId defined and a paid level: 5729\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(artist=None, auth='Logged Out', firstName=None, gender=None, itemInSession=49, lastName=None, length=None, level='paid', location=None, method='GET', page='Home', registration=None, sessionId=141, song=None, status=200, ts=1538381499000, userAgent=None, userId=''),\n",
       " Row(artist=None, auth='Logged Out', firstName=None, gender=None, itemInSession=50, lastName=None, length=None, level='paid', location=None, method='PUT', page='Login', registration=None, sessionId=141, song=None, status=307, ts=1538381500000, userAgent=None, userId=''),\n",
       " Row(artist=None, auth='Logged Out', firstName=None, gender=None, itemInSession=57, lastName=None, length=None, level='paid', location=None, method='GET', page='Home', registration=None, sessionId=141, song=None, status=200, ts=1538382349000, userAgent=None, userId='')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the rows with no userId and with a paid level\n",
    "paid_level_df = no_user_df.filter(no_user_df[\"level\"] == \"paid\")\n",
    "print(f'Number of rows with no userId defined and a paid level: {paid_level_df.count()}')\n",
    "print('Sample:')\n",
    "paid_level_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column \"auth\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(auth='Logged Out')]\n",
      "\n",
      "Column \"page\":\n",
      "\tNumber of unique values: 5\n",
      "\tSample: [Row(page='Home'), Row(page='About'), Row(page='Login'), Row(page='Help'), Row(page='Error')]\n"
     ]
    }
   ],
   "source": [
    "# Print unique auths for rows with no userId and with a paid level\n",
    "show_unique_stats(paid_level_df, ['auth', 'page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So either guests or logged out individuals don't have `userId` defined, which makes sense. It was weird to me that rows with no `userId` can have a paid level, but looks like they're coming from users that are logging out and that could just be an artifact from what remains of the user session. \n",
    "\n",
    "Given the above, I think is safe to continue just with rows that have user_id defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we're interested in user churn, let's remove rows with an empty user_id and\n",
    "# generate a separate dataframe\n",
    "df_v2 = df.filter('userId != \"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing empty user_ids: 278154\n",
      "Summary statistics for ['artist', 'auth', 'firstName', 'gender', 'itemInSession']\n",
      "+-------+------------------+---------+---------+------+------------------+\n",
      "|summary|            artist|     auth|firstName|gender|     itemInSession|\n",
      "+-------+------------------+---------+---------+------+------------------+\n",
      "|  count|            228108|   278154|   278154|278154|            278154|\n",
      "|   mean| 551.0852017937219|     null|     null|  null|114.89918174824018|\n",
      "| stddev|1217.7693079161374|     null|     null|  null|129.85172939949013|\n",
      "|    min|               !!!|Cancelled| Adelaida|     F|                 0|\n",
      "|    max| ÃÂlafur Arnalds|Logged In|   Zyonna|     M|              1321|\n",
      "+-------+------------------+---------+---------+------+------------------+\n",
      "\n",
      "Summary statistics for ['lastName', 'length', 'level', 'location', 'method']\n",
      "+-------+--------+------------------+------+-----------------+------+\n",
      "|summary|lastName|            length| level|         location|method|\n",
      "+-------+--------+------------------+------+-----------------+------+\n",
      "|  count|  278154|            228108|278154|           278154|278154|\n",
      "|   mean|    null|249.11718197783722|  null|             null|  null|\n",
      "| stddev|    null| 99.23517921058324|  null|             null|  null|\n",
      "|    min|   Adams|           0.78322|  free|       Albany, OR|   GET|\n",
      "|    max|  Wright|        3024.66567|  paid|Winston-Salem, NC|   PUT|\n",
      "+-------+--------+------------------+------+-----------------+------+\n",
      "\n",
      "Summary statistics for ['page', 'registration', 'sessionId', 'song', 'status']\n",
      "+-------+-------+--------------------+------------------+--------------------+------------------+\n",
      "|summary|   page|        registration|         sessionId|                song|            status|\n",
      "+-------+-------+--------------------+------------------+--------------------+------------------+\n",
      "|  count| 278154|              278154|            278154|              228108|            278154|\n",
      "|   mean|   null|1.535358834085557E12|1042.5616241362698|            Infinity|209.10321620397335|\n",
      "| stddev|   null| 3.291321616328068E9| 726.5010362219821|                 NaN|30.151388851327823|\n",
      "|    min|  About|       1521380675000|                 1|\u001c",
      "ÃÂg ÃÂtti Gr...|               200|\n",
      "|    max|Upgrade|       1543247354000|              2474|ÃÂau hafa slopp...|               404|\n",
      "+-------+-------+--------------------+------------------+--------------------+------------------+\n",
      "\n",
      "Summary statistics for ['ts', 'userAgent', 'userId']\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|summary|                  ts|           userAgent|            userId|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|  count|              278154|              278154|            278154|\n",
      "|   mean|1.540958915431857...|                null| 59682.02278593872|\n",
      "| stddev|1.5068287123347573E9|                null|109091.94999910519|\n",
      "|    min|       1538352117000|\"Mozilla/5.0 (Mac...|                10|\n",
      "|    max|       1543799476000|Mozilla/5.0 (comp...|                99|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics again to get a sense of impact of the removal\n",
    "print(f'Number of rows after removing empty user_ids: {df_v2.count()}')\n",
    "nice_describe(df_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column \"artist\":\n",
      "\tNumber of unique values: 17656\n",
      "\tSample: [Row(artist='The Black Keys'), Row(artist='Silverstein'), Row(artist='Kate Nash'), Row(artist='Yann Tiersen'), Row(artist=\"Jane's Addiction\")]\n",
      "\n",
      "Column \"auth\":\n",
      "\tNumber of unique values: 2\n",
      "\tSample: [Row(auth='Cancelled'), Row(auth='Logged In')]\n",
      "\n",
      "Column \"firstName\":\n",
      "\tNumber of unique values: 189\n",
      "\tSample: [Row(firstName='Maddox'), Row(firstName='Karter'), Row(firstName='Lucas'), Row(firstName='Grace'), Row(firstName='Antonina')]\n",
      "\n",
      "Column \"gender\":\n",
      "\tNumber of unique values: 2\n",
      "\tSample: [Row(gender='F'), Row(gender='M')]\n",
      "\n",
      "Column \"lastName\":\n",
      "\tNumber of unique values: 173\n",
      "\tSample: [Row(lastName='Harrison'), Row(lastName='Thornton'), Row(lastName='Pena'), Row(lastName='Jones'), Row(lastName='Santos')]\n",
      "\n",
      "Column \"level\":\n",
      "\tNumber of unique values: 2\n",
      "\tSample: [Row(level='free'), Row(level='paid')]\n",
      "\n",
      "Column \"location\":\n",
      "\tNumber of unique values: 114\n",
      "\tSample: [Row(location='Gainesville, FL'), Row(location='Atlantic City-Hammonton, NJ'), Row(location='Deltona-Daytona Beach-Ormond Beach, FL'), Row(location='San Diego-Carlsbad, CA'), Row(location='Cleveland-Elyria, OH')]\n",
      "\n",
      "Column \"method\":\n",
      "\tNumber of unique values: 2\n",
      "\tSample: [Row(method='PUT'), Row(method='GET')]\n",
      "\n",
      "Column \"page\":\n",
      "\tNumber of unique values: 19\n",
      "\tSample: [Row(page='Cancel'), Row(page='Submit Downgrade'), Row(page='Thumbs Down'), Row(page='Home'), Row(page='Downgrade')]\n",
      "\n",
      "Column \"song\":\n",
      "\tNumber of unique values: 58481\n",
      "\tSample: [Row(song='Never Gonna Be Alone (Album Version)'), Row(song='TULENLIEKKI'), Row(song='Underwaterboys'), Row(song='Saor (Free)/News From Nowhere'), Row(song='New E')]\n",
      "\n",
      "Column \"userAgent\":\n",
      "\tNumber of unique values: 56\n",
      "\tSample: [Row(userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"'), Row(userAgent='\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"'), Row(userAgent='Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:31.0) Gecko/20100101 Firefox/31.0'), Row(userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"'), Row(userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print once more unique stats for all categorical variables\n",
    "show_unique_stats(df_v2, categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for modeling\n",
    "\n",
    "Our goal is to build a model that can predict the probability of a Sparkify user churning sometime in the future given contextual information about its current state. For that we first need to define what churning means and label the users, then craft some features so we can unlock the ability to construct a supervised learning model.\n",
    "\n",
    "### Defining Churn\n",
    "\n",
    "We can define churn as the action of an user unsubscribing from the Sparkify service. During the initial exploration, the `auth` field showed it can take the value `Cancelled` and I anticipate those rows would allow us to identify users that churned. Let's look at a row in such state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist=None, auth='Cancelled', firstName='Adriel', gender='M', itemInSession=104, lastName='Mendoza', length=None, level='paid', location='Kansas City, MO-KS', method='GET', page='Cancellation Confirmation', registration=1535623466000, sessionId=514, song=None, status=200, ts=1538943990000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.77.4 (KHTML, like Gecko) Version/7.0.5 Safari/537.77.4\"', userId='18')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where('auth == \"Cancelled\"').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the user visited the `Cancellation Confirmation` page, so it sounds like it really did churn at that point. Let's explore the timeline of that user in that particular session to help us understand better what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_timeline(df, user_id, session_id, cols, n=5):\n",
    "    \"\"\"Grab the rows for a particular session of an user and prints the last n actions recorded\"\"\"\n",
    "    user_df = df.where(f'userId={user_id} AND sessionId={session_id}')\n",
    "    print(f'Number of rows for user with id {user_id} and session id {session_id}: {user_df.count()}')\n",
    "    user_df.select(cols).sort(sqlF.desc('ts')).show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for user with id 18 and session id 514: 102\n",
      "+-------------+---------+---------+--------------------+-----+-------------+\n",
      "|           ts|sessionId|     auth|                page|level|itemInSession|\n",
      "+-------------+---------+---------+--------------------+-----+-------------+\n",
      "|1538943990000|      514|Cancelled|Cancellation Conf...| paid|          104|\n",
      "|1538943740000|      514|Logged In|              Cancel| paid|          103|\n",
      "|1538943739000|      514|Logged In|           Downgrade| paid|          102|\n",
      "|1538943726000|      514|Logged In|            NextSong| paid|          101|\n",
      "|1538943440000|      514|Logged In|            NextSong| paid|          100|\n",
      "+-------------+---------+---------+--------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the timeline of actions for the user with id 18 and session with id 514\n",
    "user_timeline(df, 18, 514, ['ts', 'sessionId', 'auth', 'page', 'level', 'itemInSession'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture starts to make sense now: when the user visits the `Cancellation Confirmation` page at some point, then it follows that the user is no longer `Logged In`. We can validate that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column \"auth\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(auth='Cancelled')]\n"
     ]
    }
   ],
   "source": [
    "# Validate that when an user visits the `Cancellation Confirmation` page, then is no longer `Logged In`\n",
    "cancel_subset_df = df.where('page=\"Cancellation Confirmation\"')\n",
    "show_unique_stats(cancel_subset_df, ['auth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column \"page\":\n",
      "\tNumber of unique values: 1\n",
      "\tSample: [Row(page='Cancellation Confirmation')]\n"
     ]
    }
   ],
   "source": [
    "# Does a user with a `Cancelled` auth means it can only have visited the `Cancellation Confirmation` page?\n",
    "auth_subset_df = df.where('auth=\"Cancelled\"')\n",
    "show_unique_stats(auth_subset_df, ['page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given all the above, I think is safe to say that any user that has an auth value of `Cancelled` can be considered churned at that point. Let's work on adding a `churned` column to the dataframe which is marked with 1 if that user churned from the platform at some point, otherwise is marked as 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_churned(df):\n",
    "    \"\"\"Returns a new dataframe with the `churned` label: boolean indicating if the user churned at some point\"\"\"\n",
    "    \n",
    "    # Identify the rows with a cancelled auth state and mark those with 1, then use a window function that groups\n",
    "    # by users and puts the cancel event at the top (if any) so every row gets a one after that when we sum\n",
    "    user_cancelled = sqlF.udf(lambda x: 1 if x == 'Cancelled' else 0, IntegerType())\n",
    "    current_window = Window.partitionBy('userId').orderBy(sqlF.desc('cancelled')).rangeBetween(\n",
    "        Window.unboundedPreceding, 0)\n",
    "    churned_df = df.withColumn('cancelled', user_cancelled('auth')) \\\n",
    "        .withColumn(\"churned\", sqlF.sum('cancelled').over(current_window))\n",
    "    return churned_df.drop('cancelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the `churned` label\n",
    "df = add_label_churned(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for user with id 18 and session id 514: 102\n",
      "+-------------+---------+---------+--------------------+-----+-------------+-------+\n",
      "|           ts|sessionId|     auth|                page|level|itemInSession|churned|\n",
      "+-------------+---------+---------+--------------------+-----+-------------+-------+\n",
      "|1538943990000|      514|Cancelled|Cancellation Conf...| paid|          104|      1|\n",
      "|1538943740000|      514|Logged In|              Cancel| paid|          103|      1|\n",
      "|1538943739000|      514|Logged In|           Downgrade| paid|          102|      1|\n",
      "|1538943726000|      514|Logged In|            NextSong| paid|          101|      1|\n",
      "|1538943440000|      514|Logged In|            NextSong| paid|          100|      1|\n",
      "+-------------+---------+---------+--------------------+-----+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show once again the timeline of actions for the user with id 18 and session with id 514\n",
    "user_timeline(add_label_churned(df), 18, 514, \n",
    "              ['ts', 'sessionId', 'auth', 'page', 'level', 'itemInSession', 'churned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's get a sense of general stats for each group so we can use that for inspiration during the feature engineering phase. Below I work on crafting a function that gathers several high level statistics for a dataframe, then run on each group separately for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_relevant_stats(df, total_rows, total_users):\n",
    "    \"\"\"\"Print high level statistics to get a sense of the properties of the dataframe provided\"\"\"\n",
    "    nrows = df.count()\n",
    "    nusers = df.select('userId').dropDuplicates().count()\n",
    "    npaid = df.groupby('userId', 'level').count().where('level == \"paid\"').count()\n",
    "    print(f'Proportion of the total rows in this group: {nrows / total_rows:.2f}')\n",
    "    print(f'Proportion of the total users in this group: {nusers / total_users:.2f}')\n",
    "    print(f'Proportion of users that paid for the service at some point: {npaid / nusers:.2f}')\n",
    "    print(f'Proportion of users that have never paid for the service: {(nusers - npaid) / nusers:.2f}')\n",
    "    \n",
    "    # The average amount of songs played can be found by grabbing the rows with a song defined\n",
    "    # and then just grouping by userId to get counts for the calculation\n",
    "    avg_agg = df.where('song is not null') \\\n",
    "        .groupby('userId') \\\n",
    "        .count() \\\n",
    "        .select(\n",
    "            sqlF.mean(sqlF.col('count')).alias('avg_songs_played')\n",
    "        ).head(1)[0]\n",
    "    print(f'Avg amount of songs played per user: {avg_agg.avg_songs_played:.2f}')\n",
    "    \n",
    "    # The average amount of sessions can be calculated from getting unique duplicate pairs of user and session\n",
    "    # ids and then grouping by user to get the counts for the average calculation\n",
    "    avg_agg = df.select('userId', 'sessionId') \\\n",
    "        .dropDuplicates() \\\n",
    "        .groupby('userId') \\\n",
    "        .count() \\\n",
    "        .select(\n",
    "            sqlF.mean(sqlF.col('count')).alias('avg_sessions')\n",
    "        ).head(1)[0]\n",
    "    print(f'Avg number of sessions per user: {avg_agg.avg_sessions:.2f}')\n",
    "    \n",
    "    # Getting the average actions and lengths per session requires us to find the particular value for each\n",
    "    # one, which we can get from the max in each group. Then we can proceed to get avg per user, and then\n",
    "    # a final average across all users\n",
    "    current_window = Window.partitionBy('userId', 'sessionId').orderBy('ts')\n",
    "    avg_agg = df.withColumn(\n",
    "            'sessionLen', (sqlF.col('ts') - sqlF.first(sqlF.col('ts')).over(current_window)) / 1000.0\n",
    "        ).groupby('userId', 'sessionId') \\\n",
    "        .max() \\\n",
    "        .groupby('userId') \\\n",
    "        .avg() \\\n",
    "        .select(\n",
    "            sqlF.mean(sqlF.col('avg(max(itemInSession))')).alias('avg_user_actions'),\n",
    "            sqlF.mean(sqlF.col('avg(max(sessionLen))')).alias('avg_session_len'),\n",
    "        ).head(1)[0]\n",
    "    print(f'Avg number of actions per session: {avg_agg.avg_user_actions:.2f}')\n",
    "    print(f'Avg duration of sessions (in hours): {avg_agg.avg_session_len / (60 * 60):.2f}')\n",
    "    \n",
    "    # Getting the average amount of seconds since the first show of users is similar to the above, but this\n",
    "    # time we don't group by sessionId\n",
    "    current_window = Window.partitionBy('userId').orderBy('ts')\n",
    "    avg_agg = df.withColumn(\n",
    "            'secondsSinceFirstShow', (sqlF.col('ts') - sqlF.first(sqlF.col('ts')).over(current_window)) / 1000.0\n",
    "        ).groupby('userId') \\\n",
    "        .max() \\\n",
    "        .select(\n",
    "            sqlF.mean(sqlF.col('max(secondsSinceFirstShow)')).alias('avg_first_show'),\n",
    "        ).head(1)[0]\n",
    "    print(f'Avg number of hours since first show: {avg_agg.avg_first_show / (60 * 60):.2f}')\n",
    "    \n",
    "    # Calculate not only page visit counts but also percentage of the total for each\n",
    "    pages_agg = df.groupby('page').count()\n",
    "    npages = pages_agg.select(sqlF.sum(sqlF.col('count')).alias('npages')).head(1)[0].npages\n",
    "    per_page_agg = pages_agg.sort(sqlF.desc('count')).withColumn('proportion', sqlF.col('count') / npages)          \n",
    "    print('Stats per page:')\n",
    "    per_page_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and users in dataset: 286500, 226\n"
     ]
    }
   ],
   "source": [
    "# Get some counts for next steps\n",
    "total_rows = df.count()\n",
    "total_users = df.select('userID').dropDuplicates().count()\n",
    "print(f'Number of rows and users in dataset: {total_rows}, {total_users}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for set of non-churned users:\n",
      "Proportion of the total rows in this group: 0.84\n",
      "Proportion of the total users in this group: 0.77\n",
      "Proportion of users that paid for the service at some point: 0.75\n",
      "Proportion of users that have never paid for the service: 0.25\n",
      "Avg amount of songs played per user: 1108.17\n",
      "Avg number of sessions per user: 24.53\n",
      "Avg number of actions per session: 88.50\n",
      "Avg duration of sessions (in hours): 5.48\n",
      "Avg number of hours since first show: 1129.71\n",
      "Stats per page:\n",
      "+-------------------+------+--------------------+\n",
      "|               page| count|          proportion|\n",
      "+-------------------+------+--------------------+\n",
      "|           NextSong|191714|  0.7933999900677051|\n",
      "|               Home| 12785|0.052910162393020904|\n",
      "|          Thumbs Up| 10692| 0.04424837358671721|\n",
      "|    Add to Playlist|  5488|0.022711847572381597|\n",
      "|         Add Friend|  3641|0.015068118988892383|\n",
      "|              Login|  3241|0.013412736512771277|\n",
      "|        Roll Advert|  2966|0.012274661060438015|\n",
      "|             Logout|  2673|0.011062093396679303|\n",
      "|        Thumbs Down|  2050|0.008483835190120678|\n",
      "|          Downgrade|  1718|0.007109867734940158|\n",
      "|               Help|  1487|0.006153884354980218|\n",
      "|           Settings|  1244|0.005148239500736...|\n",
      "|              About|   868|0.003592179973182804|\n",
      "|            Upgrade|   387|0.001601582545647...|\n",
      "|      Save Settings|   252|0.001042890959956298|\n",
      "|              Error|   226|9.352910990084259E-4|\n",
      "|     Submit Upgrade|   127|5.255839361684517E-4|\n",
      "|   Submit Downgrade|    54|2.234766342763495...|\n",
      "|           Register|    18|7.449221142544985E-5|\n",
      "|Submit Registration|     5|2.069228095151385E-5|\n",
      "+-------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Stats for set of non-churned users:')\n",
    "print_relevant_stats(df.where('churned=0'), total_rows, total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for set of churned users:\n",
      "Proportion of the total rows in this group: 0.16\n",
      "Proportion of the total users in this group: 0.23\n",
      "Proportion of users that paid for the service at some point: 0.69\n",
      "Proportion of users that have never paid for the service: 0.31\n",
      "Avg amount of songs played per user: 699.88\n",
      "Avg number of sessions per user: 10.33\n",
      "Avg number of actions per session: 78.94\n",
      "Avg duration of sessions (in hours): 4.33\n",
      "Avg number of hours since first show: 564.35\n",
      "Stats per page:\n",
      "+--------------------+-----+--------------------+\n",
      "|                page|count|          proportion|\n",
      "+--------------------+-----+--------------------+\n",
      "|            NextSong|36394|  0.8112072039942939|\n",
      "|           Thumbs Up| 1859| 0.04143634094151213|\n",
      "|                Home| 1672|0.037268188302425106|\n",
      "|     Add to Playlist| 1038|0.023136590584878745|\n",
      "|         Roll Advert|  967|0.021554029957203995|\n",
      "|          Add Friend|  636|0.014176176890156919|\n",
      "|              Logout|  553| 0.01232614122681883|\n",
      "|         Thumbs Down|  496|0.011055634807417974|\n",
      "|           Downgrade|  337|0.007511590584878745|\n",
      "|            Settings|  270|0.006018188302425107|\n",
      "|                Help|  239|0.005327211126961484|\n",
      "|             Upgrade|  112|0.002496433666191...|\n",
      "|       Save Settings|   58|0.001292796005706134|\n",
      "|               About|   56|0.001248216833095...|\n",
      "|              Cancel|   52|0.001159058487874...|\n",
      "|Cancellation Conf...|   52|0.001159058487874...|\n",
      "|               Error|   32|7.132667617689016E-4|\n",
      "|      Submit Upgrade|   32|7.132667617689016E-4|\n",
      "|    Submit Downgrade|    9|2.006062767475035...|\n",
      "+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Stats for set of churned users:')\n",
    "print_relevant_stats(df.where('churned=1'), total_rows, total_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the above are pretty informative! At a high level, it looks like the more time the user spents in the platform, then the less likely is for the user to churn in the near future. Armed with this information, let's move to the feature engineering phase! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "The dataframe is not yet in a place for us to try to predict churning. What we really want is to have a single set of features per user, but currently each user can have multiple rows because they represent singular actions at points in time as opposed to the current high level characteristics of the user. We did start to summarize properties in the previous section, for example the column `churn` is something that represents current status, but we still need to do more. \n",
    "\n",
    "Based on the learnings from the previous section, I plan to concentrate in crafting the following features per user:\n",
    "\n",
    "- `number_sessions`: Total amount of sessions\n",
    "- `seconds_since_genesis`: Total amount of seconds since first appearance\n",
    "- `avg_actions_per_session`: Average amount of actions per session\n",
    "- `avg_seconds_per_session`: Average amount seconds spent per session\n",
    "\n",
    "#### Total amount of sessions\n",
    "\n",
    "This is easy, we just need to get distinct pairs of user and session ids and then group just user ids so we can extract the counts we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_number_sessions(df):\n",
    "    \"\"\"Returns a new dataframe with the `number_sessions` feature: total amount of sessions of the user\"\"\"\n",
    "    counts_df = df.select('userId', 'sessionId') \\\n",
    "        .dropDuplicates() \\\n",
    "        .groupby('userId') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count', 'number_sessions')\n",
    "    return df.join(counts_df, ['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|number_sessions|\n",
      "+------+---------------+\n",
      "|100010|              7|\n",
      "|200002|              6|\n",
      "|   125|              1|\n",
      "|   124|             29|\n",
      "|    51|             10|\n",
      "+------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add number_sessions feature column\n",
    "features_df = add_feature_number_sessions(df)\n",
    "features_df.select('userId', 'number_sessions').dropDuplicates().show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total amount of seconds since first appearance\n",
    "\n",
    "We can get the total amount of seconds since the first user appearance with a window function that calculates the delta of the current timestamp to the first one observed when grouping by user. From there, we just grab the max value obtained for each partition as the final value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_seconds_since_genesis(df):\n",
    "    \"\"\"Returns a new dataframe with the `seconds_since_genesis`: seconds since first user appearance\"\"\"\n",
    "    current_window = Window.partitionBy('userId').orderBy('ts')\n",
    "    genesis_df = df.withColumn('seconds_since_genesis', \n",
    "                               (sqlF.col('ts') - sqlF.first(sqlF.col('ts')).over(current_window)) / 1000.0)\n",
    "    genesis_df = genesis_df.groupby('userId') \\\n",
    "        .max() \\\n",
    "        .withColumnRenamed('max(seconds_since_genesis)', 'seconds_since_genesis') \\\n",
    "        .select('userId', 'seconds_since_genesis')\n",
    "    return df.join(genesis_df, ['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------+\n",
      "|userId|seconds_since_genesis|\n",
      "+------+---------------------+\n",
      "|   156|                420.0|\n",
      "|   135|               1347.0|\n",
      "|   125|               1774.0|\n",
      "|100011|               2663.0|\n",
      "|100024|               5115.0|\n",
      "+------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add hours_since_genesis feature column\n",
    "features_df = add_feature_seconds_since_genesis(df)\n",
    "features_df.select('userId', 'seconds_since_genesis').dropDuplicates().sort('seconds_since_genesis').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average amount of actions per session\n",
    "\n",
    "To calculate the average session actions per user, we need to group by both user and session id first. From there, the max `itemInSession` column will report the total amount of actions per session. Finally, we can take the average of those and we'll get the value we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_avg_actions_per_session(df):\n",
    "    \"\"\"Returns a new dataframe with the `avg_actions_per_session`: average amount of actions per session\"\"\"\n",
    "    current_window = Window.partitionBy('userId').orderBy('ts')\n",
    "    avg_df = df.groupby('userId', 'sessionId') \\\n",
    "        .max() \\\n",
    "        .groupby('userId') \\\n",
    "        .avg() \\\n",
    "        .withColumnRenamed('avg(max(itemInSession))', 'avg_actions_per_session') \\\n",
    "        .select('userId', 'avg_actions_per_session')\n",
    "    return df.join(avg_df, ['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n",
      "|userId|avg_actions_per_session|\n",
      "+------+-----------------------+\n",
      "|   135|                    5.0|\n",
      "|    90|                    8.2|\n",
      "|   125|                   10.0|\n",
      "|   134|                  12.25|\n",
      "|   156|                   14.0|\n",
      "+------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add avg_actions_per_session feature column\n",
    "features_df = add_feature_avg_actions_per_session(df)\n",
    "features_df.select('userId', 'avg_actions_per_session').dropDuplicates().sort('avg_actions_per_session').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average amount of seconds spent per session\n",
    "\n",
    "To calculate the average session duration per user we do something similar to the previous case, we just need to construct an intermediate column in the process so we can calculate the running session duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_avg_seconds_per_session(df):\n",
    "    \"\"\"Return a new dataframe with the `avg_seconds_per_session`: average amount of seconds spent per session\"\"\"\n",
    "    current_window = Window.partitionBy('userId', 'sessionId').orderBy('ts')\n",
    "    avg_df = df.withColumn(\n",
    "            'sessionLen', (sqlF.col('ts') - sqlF.first(sqlF.col('ts')).over(current_window)) / 1000.0\n",
    "        ).groupby('userId', 'sessionId') \\\n",
    "        .max() \\\n",
    "        .groupby('userId') \\\n",
    "        .avg() \\\n",
    "        .withColumnRenamed('avg(max(sessionLen))', 'avg_seconds_per_session') \\\n",
    "        .select('userId', 'avg_seconds_per_session')\n",
    "    return df.join(avg_df, ['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n",
      "|userId|avg_seconds_per_session|\n",
      "+------+-----------------------+\n",
      "|   156|                  420.0|\n",
      "|   135|                 1347.0|\n",
      "|   125|                 1774.0|\n",
      "|    90|                 1782.4|\n",
      "|   134|                2124.75|\n",
      "+------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add avg_seconds_per_session feature column\n",
    "features_df = add_feature_avg_seconds_per_session(df)\n",
    "features_df.select('userId', 'avg_seconds_per_session').dropDuplicates().sort('avg_seconds_per_session').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "\n",
    "We now have all the pieces in place to create a function that loads the dataset, cleans it, adds the features and labels, and reduces into a form in which one row is kept per user with just the features and predicting label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_for_ml(json_filepath):\n",
    "    \"\"\"Read the json dataset into a dataframe, then clean it and transform it for modeling\"\"\"\n",
    "    \n",
    "    print(f'Loading datframe from \"{json_filepath}\"...')\n",
    "    df = spark.read.json(json_filepath)\n",
    "    print(f'Loaded {df.count()} rows')\n",
    "    \n",
    "    print(f'Removing rows with a blank userId...')\n",
    "    df_clean_v1 = df.filter('userId != \"\"')\n",
    "    print(f'{df.count() - df_clean_v1.count()} rows were removed')\n",
    "                      \n",
    "    print(f'Removing rows with a null user id, session id, or timestamp...')\n",
    "    df_clean_v2 = df_clean_v1.filter('userId IS NOT NULL AND sessionId is not NULL AND ts IS NOT NULL')\n",
    "    print(f'{df_clean_v1.count() - df_clean_v2.count()} rows were removed')\n",
    "    \n",
    "    print('Adding feature \"number_sessions\"...')\n",
    "    df_with_features = add_feature_number_sessions(df_clean_v2)\n",
    "    \n",
    "    print('Adding feature \"seconds_since_genesis\"...')\n",
    "    df_with_features = add_feature_seconds_since_genesis(df_with_features)\n",
    "                   \n",
    "    print('Adding feature \"avg_actions_per_session\"...')\n",
    "    df_with_features = add_feature_avg_actions_per_session(df_with_features)\n",
    "                   \n",
    "    print('Adding feature \"avg_seconds_per_session\"...')\n",
    "    df_with_features = add_feature_avg_seconds_per_session(df_with_features)\n",
    "                   \n",
    "    print('Adding label to predict \"churned\"...')\n",
    "    df_with_features = add_label_churned(df_with_features)\n",
    "    \n",
    "    print('Reducing dataframe to one row per user...')\n",
    "    features = ['number_sessions', 'seconds_since_genesis', 'avg_actions_per_session', 'avg_seconds_per_session' ]\n",
    "    final_df = df_with_features.select(['userId', 'churned'] + features).dropDuplicates()\n",
    "    print(f'{df_with_features.count() - final_df.count()} rows were removed during the reduction')\n",
    "    \n",
    "    print(f'Final dimensions: ({final_df.count()}, {len(final_df.columns)})')\n",
    "    print(f'Dataframe is ready for modeling!')\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datframe from \"./datasets/mini_sparkify_event_data.json\"...\n",
      "Loaded 286500 rows\n",
      "Removing rows with a blank userId...\n",
      "8346 rows were removed\n",
      "Removing rows with a null user id, session id, or timestamp...\n",
      "0 rows were removed\n",
      "Adding feature \"number_sessions\"...\n",
      "Adding feature \"seconds_since_genesis\"...\n",
      "Adding feature \"avg_actions_per_session\"...\n",
      "Adding feature \"avg_seconds_per_session\"...\n",
      "Adding label to predict \"churned\"...\n",
      "Reducing dataframe to one row per user...\n",
      "277929 rows were removed during the reduction\n",
      "Final dimensions: (225, 6)\n",
      "Dataframe is ready for modeling!\n",
      "+------+-------+---------------+---------------------+-----------------------+-----------------------+\n",
      "|userId|churned|number_sessions|seconds_since_genesis|avg_actions_per_session|avg_seconds_per_session|\n",
      "+------+-------+---------------+---------------------+-----------------------+-----------------------+\n",
      "|    10|      0|              6|            3666568.0|                  135.5|     27584.833333333332|\n",
      "|   100|      0|             35|            5094687.0|      93.22857142857143|      18991.14285714286|\n",
      "|100001|      1|              4|             121701.0|                  49.25|                 8889.5|\n",
      "|100002|      0|              4|            4760031.0|                   54.0|                12071.0|\n",
      "|100003|      1|              2|             174361.0|                   41.0|                 5947.0|\n",
      "|100004|      0|             21|            4924230.0|     60.666666666666664|     11159.190476190477|\n",
      "|100005|      1|              5|            1564810.0|                   43.0|                 7211.2|\n",
      "|100006|      1|              1|               5606.0|                   45.0|                 5606.0|\n",
      "|100007|      1|              9|            5026506.0|                   58.0|     11364.666666666666|\n",
      "|100008|      0|              6|            4233555.0|                  158.5|     31695.166666666668|\n",
      "|100009|      1|             10|            2236733.0|                   69.1|                12717.7|\n",
      "|100010|      0|              7|            3820418.0|      54.42857142857143|                 9269.0|\n",
      "|100011|      1|              1|               2663.0|                   22.0|                 2663.0|\n",
      "|100012|      1|              7|            2514820.0|      86.57142857142857|     16286.285714285714|\n",
      "|100013|      1|             14|            2681750.0|                  100.5|     19799.928571428572|\n",
      "|100014|      1|              6|            3563513.0|     51.833333333333336|     11088.833333333334|\n",
      "|100015|      1|             12|            4643594.0|      89.66666666666667|     16715.666666666668|\n",
      "|100016|      0|              8|            4927081.0|                 80.875|               15924.25|\n",
      "|100017|      1|              1|              11956.0|                   74.0|                11956.0|\n",
      "|100018|      0|             21|            4634732.0|     61.904761904761905|     11591.238095238095|\n",
      "+------+-------+---------------+---------------------+-----------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# READ ME!!\n",
    "# Use the following path instead if you're running from AWS\n",
    "# file_path = 's3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json'\n",
    "\n",
    "# Use the following path only when running locally and after downloading datasets\n",
    "file_path = './datasets/mini_sparkify_event_data.json'\n",
    "\n",
    "final_df = load_df_for_ml(file_path)\n",
    "final_df.sort('userId').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Supervised Model\n",
    "\n",
    "We're finally ready to run a supervised machine learning model over the dataset crafted in the previous section. I think we should aim for a model with as high recall as possible, given how small the churned set is and also because I imagine that a false negative (predicting that someone won't churn, when in fact they will) is more damaging than a false positive (predicting someone will churn, when in fact they were not planning).\n",
    "\n",
    "Let's set up a LogisticRegression pipeline using default values for the hyperparameters to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a random 80% of the dataset for the train set and the rest for validation\n",
    "train_df, validation_df = final_df.withColumnRenamed('churned', 'label').randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ml_pipeline(clf):\n",
    "    \"\"\"Constructs a pipeline that's able to assemble, transform, and fit a LogisticRegression model\"\"\"\n",
    "    features = ['number_sessions', 'seconds_since_genesis', 'avg_actions_per_session', 'avg_seconds_per_session' ]\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    return Pipeline(stages=[assembler, clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline to the training dataset\n",
    "pipeline = get_ml_pipeline(LogisticRegression(standardization=True))\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, validation_df):\n",
    "    \"\"\"Runs a model against the provided test set and prints performance stats\"\"\"\n",
    "    results = model.transform(validation_df)    \n",
    "    predictionAndLabels = results.rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    print('Performance Stats')\n",
    "    print(f'Accuracy: {metrics.accuracy:.4f}')\n",
    "    print(f'Precision = {metrics.precision(1.0):.4f}')\n",
    "    print(f'Recall = {metrics.recall(1.0):.4f}')\n",
    "    print(f'F1 Score = {metrics.fMeasure(1.0):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Stats\n",
      "Accuracy: 0.7941\n",
      "Precision = 0.6667\n",
      "Recall = 0.2500\n",
      "F1 Score = 0.3636\n"
     ]
    }
   ],
   "source": [
    "# Run model against the validation dataset and print performance statistics\n",
    "eval_model(model, validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did good on accuracy and precision, but didn't do so well on recall (which translated into a low F1 score). Let's try to do better with cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cross_validator(numFolds=3):\n",
    "    lr = LogisticRegression(standardization=True)\n",
    "    pipeline = get_ml_pipeline(lr)\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.0, 0.5]) \\\n",
    "        .addGrid(lr.aggregationDepth, [2, 4]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 1.0]) \\\n",
    "        .addGrid(lr.maxIter, [10, 100]) \\\n",
    "        .build()\n",
    "    return CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=numFolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-fold cross-validation to try to find better hyperparameters. This will take several minutes locally...\n",
    "cv = build_cross_validator()\n",
    "cv_model = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Stats\n",
      "Accuracy: 0.7941\n",
      "Precision = 0.6667\n",
      "Recall = 0.2500\n",
      "F1 Score = 0.3636\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the cross-validated model\n",
    "eval_model(cv_model, validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It didn't really improve things, so the default values are already working well with the current train-test split. The medium dataset just had 225 distinct users, so I'm sure part of the problem is the sample size. We could try other machine learning models or try engineering more features, but I think a 25% recall is good enough to move on to try on the larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying on the large dataset\n",
    "\n",
    "We can now load the large dataset and apply the same learning pipeline as we did for the smaller dataset. **BUT BE WARNED!!**: It is recommended to be running this on a distributed Spark cluster since the large dataset is about 12GB in size. Locally this will likely fail, so I don't recommend trying. I already went through the effort of spinning up an [AWS EMR](https://aws.amazon.com/emr/) cluster for running this, so I've included the outputs from the execution below for the interest of evaluating the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large dataset and show a sample. This will take a while...\n",
    "# Note: This will break locally because the s3 path won't be reachable and that's intentional. \n",
    "#       You should not try to run any of the logic below locally\n",
    "df_large = load_df_for_ml('s3n://udacity-dsnd/sparkify/sparkify_event_data.json')\n",
    "df_large.sort('userId').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Output**:\n",
    "> ```\n",
    "Loading datframe from \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"...\n",
    "Loaded 26259199 rows\n",
    "Removing rows with a blank userId...\n",
    "0 rows were removed\n",
    "Removing rows with a null user id, session id, or timestamp...\n",
    "0 rows were removed\n",
    "Adding feature \"number_sessions\"...\n",
    "Adding feature \"seconds_since_genesis\"...\n",
    "Adding feature \"avg_actions_per_session\"...\n",
    "Adding feature \"avg_seconds_per_session\"...\n",
    "Adding label to predict \"churned\"...\n",
    "Reducing dataframe to one row per user...\n",
    "26236921 rows were removed during the reduction\n",
    "Final dimensions: (22278, 6)\n",
    "Dataframe is ready for modeling!\n",
    "+-------+-------+---------------+---------------------+-----------------------+-----------------------+\n",
    "| userId|churned|number_sessions|seconds_since_genesis|avg_actions_per_session|avg_seconds_per_session|\n",
    "+-------+-------+---------------+---------------------+-----------------------+-----------------------+\n",
    "|1000025|      1|             17|            1424016.0|     120.52941176470588|      24287.58823529412|\n",
    "|1000035|      0|             22|            3941067.0|      73.04545454545455|     14156.181818181818|\n",
    "|1000083|      1|             11|             966433.0|      55.45454545454545|     11166.272727272728|\n",
    "|1000103|      0|              4|            4095026.0|                   24.0|                 4136.0|\n",
    "|1000164|      0|             18|            5181168.0|      64.94444444444444|     13133.388888888889|\n",
    "|1000168|      0|              8|            3528932.0|                   86.5|              16723.875|\n",
    "|1000182|      0|              4|            1818146.0|                  111.0|                21725.0|\n",
    "|1000194|      0|              3|            2255280.0|                   30.0|                 5688.0|\n",
    "|1000214|      0|             27|            5048076.0|      93.33333333333333|     18099.814814814814|\n",
    "|1000233|      0|              5|            3086597.0|                   79.6|                17645.4|\n",
    "|1000244|      0|              2|            4349239.0|                   11.5|                 1982.0|\n",
    "|1000248|      0|             15|            5000407.0|     161.46666666666667|     36214.933333333334|\n",
    "|1000280|      1|             22|            3721747.0|      60.22727272727273|     11694.363636363636|\n",
    "|1000353|      1|              4|            1895039.0|                  87.25|               15133.25|\n",
    "|1000407|      0|             25|            5182334.0|                   87.8|               17619.84|\n",
    "|1000409|      0|             29|            5107350.0|     102.72413793103448|     20855.827586206895|\n",
    "|1000446|      0|              6|            3988633.0|     29.833333333333332|      5354.166666666667|\n",
    "|1000503|      1|              3|             877300.0|      73.33333333333333|     15416.666666666666|\n",
    "|1000527|      0|             10|            4609617.0|                   69.2|                12802.3|\n",
    "|1000611|      0|             26|            5010379.0|       68.6923076923077|     13509.461538461539|\n",
    "+-------+-------+---------------+---------------------+-----------------------+-----------------------+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a random 80% of the dataset for the train set and the rest for validation\n",
    "large_train_df, large_validation_df = df_large.withColumnRenamed('churned', 'label').randomSplit(\n",
    "    [0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-fold cross-validation to try to find better hyperparameters\n",
    "cv = build_cross_validator()\n",
    "cv_model = cv.fit(large_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the cross-validated model\n",
    "results = cv_model.transform(validation)\n",
    "results = eval_model(cv_model, validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Output**:\n",
    ">```\n",
    "Performance Stats\n",
    "Accuracy: 0.7941\n",
    "Precision = 1.0000\n",
    "Recall = 0.1250\n",
    "F1 Score = 0.2222\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, we had perfect precision but pretty bad recall! So basically our model didn't have any false positives, meaning that whenever it predicted that someone would churn it was pretty certain about it. However it had tons of false negatives, which means it missed out on many others that churned. \n",
    "\n",
    "This could be an indication of a lack of features in our model, so a good step from here could be to go back to the feature engineering phase. Maybe we can extract more info from the particular pages visited before churning, or maybe the fact that the user paid or not for the service is also a strong indicator. But for the purpose of this work, I would call this a successful first iteration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This work demonstrated how pyspark can be effectively used to both execute a data exploration and machine learning task over large scale data. It started with working with a medium dataset locally just so we can iterate faster and gain confidence on the work. It then shared the results of running the same code on large scale dataset using a distributed cluster powered by AWS EMR.\n",
    "\n",
    "The results of the prediction task were encouraging: we were able to achieve a model with high precision. But we also discovered its recall ability was pretty weak, leaving room for improvement. In particular, the best next step from here could be to go back to the feature engineering phase and try to get more context from the user actions. Maybe we could extract more info from the particular pages visited before churning, or maybe the fact that the user paid or not for the service is also a strong indicator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
